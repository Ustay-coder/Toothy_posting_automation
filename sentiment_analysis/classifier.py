# toothpaste_review_analyzer.py
from pathlib import Path
from typing import List, Tuple, Dict, Literal
import os, re, html, openai, pandas as pd
from collections import defaultdict
import time, gc, psutil, torch
import openai
from sentence_transformers import SentenceTransformer
from sklearn.cluster import AgglomerativeClustering
import numpy as np
from .utils import clean_html, split_korean_sentences


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) SBERT + AgglomerativeClustering
class SentenceOpinionClustering:
    """
    â‘  SBERT ì„ë² ë”©ìœ¼ë¡œ ë¬¸ì¥ êµ°ì§‘í™”
    â‘¡ ê° êµ°ì§‘ì—ì„œ ëŒ€í‘œ ê°ì • êµ¬ ì¶”ì¶œ(GPT-4)
    â‘¢ ê²°ê³¼(dict)Â·ì¶œë ¥(print) ì§€ì›
    """

    def __init__(
        self,
        *,
        sbert_model: str = "snunlp/KR-SBERT-V40K-klueNLI-augSTS",
        distance_threshold: float = 0.4,
        openai_model: str = "gpt-4o-mini",
        openai_temperature: float = 0.3,
        device: str = "cpu",
        profile: bool = False,
        purpose: str = "ë‚˜ëŠ” ì´ ë¦¬ë·°ë¥¼ í†µí•´ ì†Œë¹„ìë“¤ì´ í•´ë‹¹ ì œí’ˆì„ ì‚¬ìš©í–ˆì„ ë•Œ ì–´ë– í•œ ê²½í—˜ì„ í–ˆëŠ”ì§€ë¥¼ íŒŒì•…í•˜ê³  ì‹¶ë‹¤."
    ):
        self.device = torch.device(device)
        self.embedder = SentenceTransformer(sbert_model, device=self.device)
        self.distance_threshold = distance_threshold
        self.openai_model = openai_model
        self.openai_temperature = openai_temperature
        self.profile = profile
        self.purpose = purpose


        # í™˜ê²½ë³€ìˆ˜ì— OPENAI_API_KEY ê°€ ì„¤ì •ë¼ ìˆì–´ì•¼ í•¨
        openai.api_key = os.getenv("OPENAI_API_KEY")

        if not openai.api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

     # ----------------------------------------------------------------
     # gpu / cpu ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ ë¶„ì„ì„ ìœ„í•œ snapshot
    def _snapshot(self):
        """(RSS, GPU alloc, GPU reserved) MB"""
        rss = psutil.Process().memory_info().rss / 1024**2
        if self.device.type == "cuda":
            torch.cuda.synchronize()
            alloc = torch.cuda.memory_allocated() / 1024**2
            reserv = torch.cuda.memory_reserved() / 1024**2
            return rss, alloc, reserv
        return rss, None, None
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1) ë¬¸ì¥ êµ°ì§‘í™”
    def _cluster_sentences(self, sentences: List[str]) -> Dict[int, List[str]]:

        if self.profile:
            rss0, g0a, g0r = self._snapshot()
            t0 = time.perf_counter()

        embeddings = self.embedder.encode(sentences, convert_to_numpy=True, batch_size=128, show_progress_bar=False)
        if self.profile:
            torch.cuda.synchronize() if self.device.type == "cuda" else None
            t1 = time.perf_counter()
            rss1, g1a, g1r = self._snapshot()
            print(
                f"[{self.device}]  embed {len(sentences):,} ë¬¸ì¥  |  "
                f"time {t1 - t0:.2f}s  |  RAM {rss1 - rss0:+.1f} MB  "
                + (
                    f"|  VRAM {g1a - g0a:+.1f}/{g1r - g0r:+.1f} MB"
                    if self.device.type == "cuda" else ""
                )
            )
        clustering = AgglomerativeClustering(
            n_clusters=None,
            distance_threshold=self.distance_threshold,
            metric="cosine",
            linkage="average",
        ).fit(embeddings)

        clusters: Dict[int, List[str]] = defaultdict(list)
        for sent, label in zip(sentences, clustering.labels_):
            clusters[label].append(sent)
        return dict(clusters)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 2) GPT-4 ë¡œ ëŒ€í‘œ ê°ì • êµ¬ ì¶”ì¶œ
    def _extract_phrase(
        self,
        cluster_sentences: List[str],
    ) -> str:
        """
        ì£¼ì–´ì§„ í´ëŸ¬ìŠ¤í„°ì—ì„œ purpose ë¥¼ ê°€ì¥ ì˜ ë“œëŸ¬ë‚´ëŠ” ê°ì • êµ¬ì ˆì„ 1ì¤„ë¡œ ì¶”ì¶œ
        """
        representative = cluster_sentences[0]           # í•„ìš”í•˜ë©´ ì¤‘ì‹¬ ë¬¸ì¥ìœ¼ë¡œ êµì²´

        prompt = (
            f"[Purpose]\n{self.purpose}\n\n"
            "ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ìœ„ ëª©ì ì„ ê°€ì¥ ì˜ ë³´ì—¬ì£¼ëŠ” ê°ì •ì„ ë‹´ì€ **í•µì‹¬ êµ¬ì ˆ í•˜ë‚˜**ë§Œ ì¶”ì¶œí•´ì¤˜.\n"
            f"\"{representative}\"\n"
            "í•œ ë¬¸ì¥ ë˜ëŠ” ì•„ì£¼ ì§§ì€ êµ¬ë¡œë§Œ."
        )

        res = openai.chat.completions.create(
            model=self.openai_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=self.openai_temperature,
        )
        return res.choices[0].message.content.strip()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 3) ê³µê°œ API
    def summarize(self, sentences: List[str]) -> Dict[int, str]:
        """
        ì…ë ¥ ë¬¸ì¥ â†’ {í´ëŸ¬ìŠ¤í„° ID: ëŒ€í‘œ ê°ì • êµ¬} ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
        """
        clusters = self._cluster_sentences(sentences)
        summary = {cid: self._extract_phrase(sents) for cid, sents in clusters.items()}
        return clusters, summary

    def print_summary(self, sentences: List[str], *, sort_by_size: bool = False) -> None:
        """
        summarize() ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì½˜ì†”ì— ì¶œë ¥
        """
        clusters = self._cluster_sentences(sentences)
        items = clusters.items()
        if sort_by_size:
            items = sorted(items, key=lambda kv: len(kv[1]), reverse=True)

        for cid, sents in items:
            phrase = self._extract_phrase(sents)
            joined = ", ".join(sents)
            print(f"ğŸ§¾ Cluster {cid} ({len(sents)}ê°œ)\n  â€¢ ëŒ€í‘œ êµ¬: {phrase}\n  â€¢ ë¬¸ì¥ë“¤: {joined}\n")
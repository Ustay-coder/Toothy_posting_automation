# toothpaste_review_analyzer.py
from pathlib import Path
from typing import List, Tuple, Dict, Literal
import pandas as pd
import openai
from classifier import SentenceOpinionClustering
import numpy as np
import os
from dotenv import load_dotenv
from utils import clean_html, split_korean_sentences

load_dotenv()
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) ë©”ì¸ ë¶„ì„ í´ë˜ìŠ¤
class ReviewAnalyzer:
    """
    blog_full.csv â†’ ê¸ì •/ë¶€ì • TOP-k í´ëŸ¬ìŠ¤í„° ì¶”ì¶œ
    """

    def __init__(
        self,
        csv_path: str | Path = "blog_full.csv",
        *,
        encoding: str | None = "utf-8-sig",
        distance_threshold: float = 0.4,
        lang: Literal["ko", "en"] = "ko",
        top_k: int = 5,
        openai_model: str = "gpt-4o-mini",
        device: str = "cpu",
        profile: bool = False,
        purpose: str = "ë‚˜ëŠ” ì´ ë¦¬ë·°ë¥¼ í†µí•´ ì†Œë¹„ìë“¤ì´ í•´ë‹¹ ì œí’ˆì„ ì‚¬ìš©í–ˆì„ ë•Œ ì–´ë– í•œ ê²½í—˜ì„ í–ˆëŠ”ì§€ë¥¼ íŒŒì•…í•˜ê³  ì‹¶ë‹¤."
    ):
        self.csv_path = Path(csv_path)
        self.profile = profile
        self.device = device
        self.encoding = encoding
        self.lang = lang
        self.top_k = top_k
        self.purpose = purpose
        self.clusterer = SentenceOpinionClustering(
            distance_threshold=distance_threshold,
            device=device,
            profile=True,
            purpose=self.purpose
        )
        self.openai_model = openai_model
        openai.api_key = os.getenv("OPENAI_API_KEY")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°ì´í„° ë¡œë“œ & ë¬¸ì¥ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _load_sentences(self, *, min_tokens: int = 2) -> Tuple[pd.DataFrame, List[str]]:
        df = pd.read_csv(
            self.csv_path,
            usecols=["title", "content", "date", "url"],
            encoding=self.encoding,
            dtype={"title": str, "content": str, "url": str},
            parse_dates=["date"],
            infer_datetime_format=True,
        )
        for col in ["title", "content"]:
            df[col] = df[col].apply(clean_html)

        df["sentences"] = df["content"].apply(split_korean_sentences)
        all_sents = [s for lst in df["sentences"] for s in lst if len(s.split()) >= min_tokens]
        return df, all_sents

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GPT ê°ì • ë¶„ë¥˜ util â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @staticmethod
    def _normalize(lbl: str, lang: str) -> str:
        lbl = lbl.strip().lower()
        ko = {"ê¸ì •": "ê¸ì •", "ì¤‘ë¦½": "ì¤‘ë¦½", "ë¶€ì •": "ë¶€ì •",
              "positive": "ê¸ì •", "neutral": "ì¤‘ë¦½", "negative": "ë¶€ì •"}
        en = {"positive": "positive", "neutral": "neutral", "negative": "negative",
              "ê¸ì •": "positive", "ì¤‘ë¦½": "neutral", "ë¶€ì •": "negative"}
        return (ko if lang == "ko" else en).get(lbl, "ì¤‘ë¦½" if lang == "ko" else "neutral")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GPT ê°ì • ë¶„ë¥˜ util (ì—…ë°ì´íŠ¸) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _classify(self, phrase: str, purpose: str) -> str:
        """
        â€¢ purpose ë¬¸ë§¥ì— ë¹„ì¶”ì–´ phrase ê°€ ê¸ì •/ì¤‘ë¦½/ë¶€ì •ì¸ì§€ íŒì •
        â€¢ purpose ì™€ ë¬´ê´€í•˜ê±°ë‚˜ ì¹˜ì•½ ì‚¬ìš© ê²½í—˜ì´ ëª…í™•ì¹˜ ì•Šìœ¼ë©´ ì¤‘ë¦½
        â€¢ self.lang == 'ko' â†’ í•œê¸€ ë¼ë²¨, 'en' â†’ ì˜ì–´ ë¼ë²¨
        """
        # â‘  system í”„ë¡¬í”„íŠ¸: ë¼ë²¨ ì–¸ì–´ ê³ ì • + ê·œì¹™ ëª…ì‹œ
        if self.lang == "en":
          sys_msg = (
            "You are a sentiment classifier focused on toothpaste reviews.\n"
            "Goal: Reflect the user's purpose, then respond with **exactly** "
            "'positive', 'neutral' or 'negative' (lowercase only).\n"
            "- If the sentence does not clearly describe the consumer's "
            "experience *after using* the toothpaste or is irrelevant to the purpose, "
            "return 'neutral'.\n"
            "- No other words, no punctuation."
          )
        else:  # 'ko'
          sys_msg = (
            "ë‹¹ì‹ ì€ ì¹˜ì•½ ë¦¬ë·°ì— ëŒ€í•œ ê°ì • ë¶„ë¥˜ê¸°ì…ë‹ˆë‹¤.\n"
            "ì•„ë˜ ëª©ì ì„ ë°˜ì˜í•œ ë’¤ ë°˜ë“œì‹œ 'ê¸ì •', 'ì¤‘ë¦½', 'ë¶€ì •' "
            "ì¤‘ í•˜ë‚˜ë§Œ ì •í™•íˆ ì¶œë ¥í•˜ì„¸ìš” (ë‹¤ë¥¸ ë¬¸ì¥Â·ê¸°í˜¸ ê¸ˆì§€).\n"
            "- ë¬¸ì¥ì´ ì†Œë¹„ìì˜ ì‚¬ìš© í›„ ê²½í—˜ì„ ëª…í™•íˆ ë§í•˜ì§€ ì•Šê±°ë‚˜ ëª©ì ê³¼ ê´€ë ¨ ì—†ìœ¼ë©´ 'ì¤‘ë¦½'ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤."
        )

        # â‘¡ user í”„ë¡¬í”„íŠ¸: ëª©ì  + ë¬¸ì¥
        user_prompt = (
            f"[Purpose]\n{purpose}\n\n"
            f"[Sentence]\n\"{phrase}\""
        )

        resp = openai.ChatCompletion.create(
            model=self.openai_model,
            messages=[
                {"role": "system", "content": sys_msg},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0,
        )
        return self._normalize(resp.choices[0].message.content, self.lang)


    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì „ì²´ íŒŒì´í”„ë¼ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def analyze(
        self,
        purpose: str,
        *,
        debug: bool = True,
        sample_size: int = 10,
        min_tokens: int = 2,
    ) -> Tuple[List[Tuple[int, str, int]], List[Tuple[int, str, int]]]:
        """
        ë°˜í™˜: (ê¸ì • TOP-k ë¦¬ìŠ¤íŠ¸, ë¶€ì • TOP-k ë¦¬ìŠ¤íŠ¸)
        ê° ì›ì†Œ: (cluster_id, ëŒ€í‘œ êµ¬, í´ëŸ¬ìŠ¤í„° ë¬¸ì¥ ìˆ˜)
        """
        _, sentences_all = self._load_sentences(min_tokens=min_tokens)
        sentences = sentences_all[:sample_size] if debug else sentences_all

        # clusters = self.clusterer.cluster(sentences)
        # phrases = {cid: self.clusterer.representative_phrase(sents)
        #            for cid, sents in clusters.items()}
        clusters, phrases = self.clusterer.summarize(sentences)

        bucket_pos, bucket_neg = [], []
        for cid, phrase in phrases.items():
            lbl = self._classify(phrase, purpose)
            if lbl in ("positive", "ê¸ì •"):
                bucket_pos.append((cid, phrase, len(clusters[cid])))
            elif lbl in ("negative", "ë¶€ì •"):
                bucket_neg.append((cid, phrase, len(clusters[cid])))
            # ì¤‘ë¦½ì€ ë²„ë¦¼

        bucket_pos.sort(key=lambda x: x[2], reverse=True)
        bucket_neg.sort(key=lambda x: x[2], reverse=True)

        return bucket_pos[:self.top_k], bucket_neg[:self.top_k]

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²°ê³¼ ì½˜ì†” ì¶œë ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def print_summary(self, pos: List[Tuple[int, str, int]], neg: List[Tuple[int, str, int]]) -> None:
        label_pos, label_neg = (("positive", "negative") if self.lang == "en" else ("ê¸ì •", "ë¶€ì •"))
        print(f"\nğŸŸ¢ {label_pos.upper()} TOP-{len(pos)}")
        for cid, phr, sz in pos:
            print(f"[{cid}] ({sz}ë¬¸ì¥) {phr}")
        print(f"\nğŸ”´ {label_neg.upper()} TOP-{len(neg)}")
        for cid, phr, sz in neg:
            print(f"[{cid}] ({sz}ë¬¸ì¥) {phr}")

    def save_summary(self, pos: List[Tuple[int, str, int]], neg: List[Tuple[int, str, int]]) -> None:
        label_pos, label_neg = (("positive", "negative") if self.lang == "en" else ("ê¸ì •", "ë¶€ì •"))
        with open(f"{self.csv_path.stem}_summary.txt", "w") as f:
            f.write(f"ğŸŸ¢ {label_pos.upper()} TOP-{len(pos)}\n")
            for cid, phr, sz in pos:
                f.write(f"[{cid}] ({sz}ë¬¸ì¥) {phr}\n")
            f.write(f"\nğŸ”´ {label_neg.upper()} TOP-{len(neg)}\n")
            for cid, phr, sz in neg:
                f.write(f"[{cid}] ({sz}ë¬¸ì¥) {phr}\n")


        